{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eozenen4g-RS"
   },
   "source": [
    "## Initial Setup\n",
    "- Install necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1743897870737,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 360
    },
    "id": "7EgW3tCrZkyb"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Load notebook (make sure the file is uploaded first)\n",
    "with open(\"hw4.ipynb\", \"r\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove global widget metadata\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "\n",
    "# Remove from each cell\n",
    "for cell in nb.cells:\n",
    "    if \"metadata\" in cell:\n",
    "        cell[\"metadata\"].pop(\"widgets\", None)\n",
    "\n",
    "# Save the cleaned version\n",
    "with open(\"Fine_Tune_Llama_Trump.ipynb\", \"w\") as f:\n",
    "    nbformat.write(nb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14251,
     "status": "ok",
     "timestamp": 1743897689120,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 360
    },
    "id": "aA2536rGZp3j",
    "outputId": "1f3274dc-78fb-4586-9ed8-c8326e264b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1743897818604,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 360
    },
    "id": "O5ivMrQzaMiN",
    "outputId": "5426daf4-be39-4bf6-8429-8e832306430a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/SysAI/Tweet_Like_Trump\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/SysAI/Tweet_Like_Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77651,
     "status": "ok",
     "timestamp": 1739136215485,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "sC2yFBGnhFFK",
    "outputId": "6be7c47b-f33c-43ae-d757-4b5053854ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.3.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.14.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, bitsandbytes, trl\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.2 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.14.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "%pip install torch datasets trl peft transformers bitsandbytes huggingface_hub wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMLx9s7EDfl_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from transformers import pipeline, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26040,
     "status": "ok",
     "timestamp": 1739136265148,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "vqWSepwXOx05",
    "outputId": "6e751742-2384-4c1b-b2c2-a28f79e864b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739136267412,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "TF82V9R2O8Kg",
    "outputId": "ba08409a-0240-438f-a149-b295f6354aa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/SysAI/Tweet_Like_Trump\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/SysAI/Tweet_Like_Trump/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95ChlPWy_NbS"
   },
   "outputs": [],
   "source": [
    "exec(open(\"secrets.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1739136278486,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "q9bk9aUB-7eW",
    "outputId": "30a28ae5-f519-4cf4-da5e-8e77d567618a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/SysAI/Tweet_Like_Trump\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUQKgUIhDh9C"
   },
   "outputs": [],
   "source": [
    "model_id: str = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8WtjSOgDxl3"
   },
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation: str = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z6Tcy-nD_BP"
   },
   "source": [
    "### Quantization and loading the model\n",
    "\n",
    "HugggingFace supports [quantization](https://huggingface.co/docs/transformers/v4.46.0/quantization/overview) using the [bits and bytes quantization](https://huggingface.co/docs/transformers/v4.46.0/quantization/bitsandbytes) which makes it easy to used quantized methods like QLoRA during fine-tuning. `bitsandbytes` computes using `fp16` for values that can't be reprsented in `int8`. We'll use the `nf4` quantization used in the QLoRA paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "d082226ac274438d8fa1c983007cbaaf",
      "0cb47322a1d645afbbaccb82586fb168",
      "d9b4c22bd3d24630b2e4cae01c50f2aa",
      "cad489391e3d45789922e0f954b336e5",
      "e2964ac45afe4058bcfb03ebb990856e",
      "ff2e9abf9dfb44e38a043559b655539c",
      "fb8885980bb2403cb53f80a17621808a",
      "ade529454435475fb9146c8e4c5bc6c0",
      "aa824339814c446689030f512ae05d35",
      "b69d00aa3f0d47eebbe8d78466e47ca5",
      "1b9fea1d938942218b91b7fa96911308",
      "0c49e735e7614fd7bd91f389323e753c",
      "a9748c5298d343d3b2f42f2e82fdd114",
      "c8d477cc05d04431b7168aadd1c9476d",
      "2b265c401c3f48769d0cdae5ff32686c",
      "5a9ae433623e443e80ed9fcff247a361",
      "62749a260c52404aae7058a9473e5968",
      "c5257d4a5bf74e2183e1a941d4f73494",
      "180ab8e21ad64eb5bc0a93d6da35e170",
      "4182f0f82fcd4d64a82b55d4b4c833b7",
      "d365e1ac20fa43a1ad0f83cd5b2d86f5",
      "d73465b5fc0b4c7d9ed35606ff2f10e6",
      "8b3c43c2afc34251afe83ff3c3319de6",
      "40e8a5f4720c4ad683de39b48d2efbcb",
      "d884c3fb1c6942589cc1a8732fca945d",
      "b3955913b6df42d6bc52d6523566cb95",
      "6c17738bc9a647ee8e696f19fcfe9a15",
      "142a71ab890842b39cfd5dcae43642eb",
      "5598d3a15c59447e974cee65d11f276f",
      "ed1cb6f0f7094b7eaf4abdb527313bfd",
      "2b43b1fd948b476997a567dc16a69874",
      "987de1ee5e2c4f3b82c0a0560c36a9a2",
      "8dad0df804f64a2a91c4c21eab7f06a7"
     ]
    },
    "executionInfo": {
     "elapsed": 63456,
     "status": "ok",
     "timestamp": 1739136355124,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "JxZNudypEAGF",
    "outputId": "d5967c2b-7fc2-4ac5-ce57-a083df67b6ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d082226ac274438d8fa1c983007cbaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c49e735e7614fd7bd91f389323e753c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3c43c2afc34251afe83ff3c3319de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooAq-ZESEGII"
   },
   "source": [
    "#### Define & patch tokenizer\n",
    "\n",
    "The Llama3.2 models don't define the padding token which is used to extend the query context. There are various suggests of how to use an existing token to do this and some use and \"end of sentence\" token. Other sources point to the \"finetune_right_pad\" token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "fb096d2cf97847e295e783d50a1e588e",
      "3bc47ffa77e44c77b416915b4d02c419",
      "b9512c6a750d44b78da1073280f80ed1",
      "a775f4d198124d0294d69d02be5ee83f",
      "13d65c2878a74ad0a0263aac677b54ea",
      "2b746ed72b2e4f5eb5eeb093cc1eba57",
      "e00280499c9f4340802a29038a841f93",
      "a9551b18e10341ecbebdfe4090b0242b",
      "d1d0fa1d143245dd95fc8c3014f13414",
      "d93256bc002c4103a8eec1319aa55641",
      "a58285f7fa49467db3ff74b2fe72aef0",
      "e48e1a447457470697388ce50a63dcc6",
      "ea8a72b41ea4487c956e8b0a574ec4b7",
      "c1ce350c2fbe49e48b6016284a7307ce",
      "fe311c49a1c34b23a36ea0541d91698f",
      "480599236166415e9704323bec1b9c3a",
      "f627a290595a4d7c980debe97b66675f",
      "c9ba40e4de334e7cb754671e1d919549",
      "35779be5dea0413b901e05c964fc7366",
      "93e2286b50f445e79a847e914ce56e4f",
      "c06ef255569d4703bbf69c6377d20570",
      "876bb53af91541e9a8ca59067e287b40",
      "a366edb841a1422290e81d58aab207d0",
      "25c88d80aab544048dd77c013e26990b",
      "20fc6ac7c4ac48218c30114bdd3c9385",
      "f22eaeeacbb14430a2f2b48ac8d3b2df",
      "7c1a9b564c9c4883828046060426aab5",
      "b9eed1533cbc4e5ba79b92e536a86708",
      "f624be8ec9e94291a27867ece8a359e8",
      "8d1130122f4a4088a555f611c0633fbf",
      "f09634bea0e3456b867430314a660978",
      "740d50ffea724b1e8ee4240c2e5c3657",
      "d46b8b12328d41cf93b8f0422d389880"
     ]
    },
    "executionInfo": {
     "elapsed": 3703,
     "status": "ok",
     "timestamp": 1739136361689,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "Y1loJ1fCEHFO",
    "outputId": "da516f2a-10cd-4251-b188-d3651e36a015"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb096d2cf97847e295e783d50a1e588e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48e1a447457470697388ce50a63dcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a366edb841a1422290e81d58aab207d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(tokenizer.pad_token)\n",
    "if not tokenizer.pad_token:\n",
    "  tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "if not model.config.pad_token_id:\n",
    "  model.config.pad_token_id = \"<|finetune_right_pad_id|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GktRtzdKEan3"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mpQq9mlDJ3H"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url=\"https://drive.google.com/file/d/1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6/view?usp=sharing\"\n",
    "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url)\n",
    "df.rename(columns={'text':'tweet_text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gWqNPrxEdHe"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# dataset_path = '/content/drive/MyDrive/Colab Notebooks/datasets/trump_tweets_cleaned.csv'\n",
    "dataset_path = \"/content/trump_tweets_cleaned.csv\"\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1739136379852,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "9SzYxsESOP8f",
    "outputId": "e1ee2936-3b38-404a-a28f-4013f0077eda"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 56571,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 382661565492125248,\n        \"min\": 1698308935,\n        \"max\": 1347569870578266115,\n        \"num_unique_values\": 56571,\n        \"samples\": [\n          557353233342296064,\n          663060539409629188,\n          263730679819423744\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 56118,\n        \"samples\": [\n          \".@megynkelly must have had a terrible vacation, she is really off her game. Was afraid to confront Dr. Cornel West. No clue on immigration!\",\n          \"If we let Crooked run the govt, history will remember 2017 as the year America lost its independence. #DrainTheSwamp https://t.co/VpYiO8CnXQ\",\n          \"Biden refuses to answer questions on packing the Supreme Court. He says voters \\u201cdon\\u2019t deserve\\u201d to know. If they win, Dems will pack the #SCOTUS w/ radical left justices who will shred the #2A, empower violent mobs, and shield deadly criminals &amp; terrorists. https://t.co/alVSZUDc73\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"isRetweet\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"t\",\n          \"f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"isDeleted\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"t\",\n          \"f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"device\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"TweetDeck\",\n          \"Twitter Mirror for iPad\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorites\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57815,\n        \"min\": 0,\n        \"max\": 1869706,\n        \"num_unique_values\": 22120,\n        \"samples\": [\n          71458,\n          21596\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13306,\n        \"min\": 0,\n        \"max\": 408866,\n        \"num_unique_values\": 20460,\n        \"samples\": [\n          10293,\n          2608\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 56022,\n        \"samples\": [\n          \"2020-07-23 12:14:58\",\n          \"2019-11-30 23:38:35\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"isFlagged\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"t\",\n          \"f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-8988fada-f5a1-4ac0-a8be-cc9acda43d89\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>isDeleted</th>\n",
       "      <th>device</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>isFlagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98454970654916608</td>\n",
       "      <td>Republicans and Democrats have both created ou...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>49</td>\n",
       "      <td>255</td>\n",
       "      <td>2011-08-02 18:07:48</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1234653427789070336</td>\n",
       "      <td>I was thrilled to be back in the Great city of...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>73748</td>\n",
       "      <td>17404</td>\n",
       "      <td>2020-03-03 01:34:50</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1218010753434820614</td>\n",
       "      <td>RT @CBS_Herridge: READ: Letter to surveillance...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>7396</td>\n",
       "      <td>2020-01-17 03:22:47</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1304875170860015617</td>\n",
       "      <td>The Unsolicited Mail In Ballot Scam is a major...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>80527</td>\n",
       "      <td>23502</td>\n",
       "      <td>2020-09-12 20:10:58</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1218159531554897920</td>\n",
       "      <td>RT @MZHemingway: Very friendly telling of even...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>9081</td>\n",
       "      <td>2020-01-17 13:13:59</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8988fada-f5a1-4ac0-a8be-cc9acda43d89')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8988fada-f5a1-4ac0-a8be-cc9acda43d89 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8988fada-f5a1-4ac0-a8be-cc9acda43d89');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-aa10014a-8bf6-4b58-a2bc-1a6690ed5380\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa10014a-8bf6-4b58-a2bc-1a6690ed5380')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-aa10014a-8bf6-4b58-a2bc-1a6690ed5380 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                    id                                         tweet_text  \\\n",
       "0    98454970654916608  Republicans and Democrats have both created ou...   \n",
       "1  1234653427789070336  I was thrilled to be back in the Great city of...   \n",
       "2  1218010753434820614  RT @CBS_Herridge: READ: Letter to surveillance...   \n",
       "3  1304875170860015617  The Unsolicited Mail In Ballot Scam is a major...   \n",
       "4  1218159531554897920  RT @MZHemingway: Very friendly telling of even...   \n",
       "\n",
       "  isRetweet isDeleted              device  favorites  retweets  \\\n",
       "0         f         f           TweetDeck         49       255   \n",
       "1         f         f  Twitter for iPhone      73748     17404   \n",
       "2         t         f  Twitter for iPhone          0      7396   \n",
       "3         f         f  Twitter for iPhone      80527     23502   \n",
       "4         t         f  Twitter for iPhone          0      9081   \n",
       "\n",
       "                  date isFlagged  \n",
       "0  2011-08-02 18:07:48         f  \n",
       "1  2020-03-03 01:34:50         f  \n",
       "2  2020-01-17 03:22:47         f  \n",
       "3  2020-09-12 20:10:58         f  \n",
       "4  2020-01-17 13:13:59         f  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y42RhzAvL7HS"
   },
   "outputs": [],
   "source": [
    "df = df[['tweet_text']]\n",
    "dataset_tweets = Dataset.from_pandas(df) # Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "raw_dataset = DatasetDict({\"train\": dataset_tweets})\n",
    "raw_dataset = raw_dataset[\"train\"].train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1739136383327,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "5ctYXZl2OuZB",
    "outputId": "f4496407-3817-4cdf-835d-ab049b6feb3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet_text'],\n",
       "        num_rows: 53742\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweet_text'],\n",
       "        num_rows: 2829\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2Zli9fpfmJL"
   },
   "source": [
    "## Prompt engineering for llama 3.2 1b\n",
    "##### GOAL: develop prompt for the model to respond like Donald Trump would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G0BX4z4fhFh"
   },
   "outputs": [],
   "source": [
    "instruction: str = \"\"\"You are a famous Hollywood male actor preparing for a role in a movie where you will be playing Donald Trump.\n",
    "    You will be asked questions and need to reply as Donald Trump would.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "93f35935770f47a8bce8fee307750e31",
      "38d84529fb094fa4889e28f08a21a3e7",
      "952407f2decd48b2aca336540a0082db",
      "be432cbaf03b4aa28b99bb8c57670b5f",
      "b89224fde9e14fc5ac4c614b4178916b",
      "d42202851b5245a9ac43e76fc96ade99",
      "e940db30bff849fb9df1914a2418de0c",
      "31d0a8274e9a433cb71bca0ded65937a",
      "ce80ec5f22584de38ad14e0b1ea04adc",
      "5a92f8a66f5b4fd58c4eac00e0fe0381",
      "8c94b598bec24abdbae568445bf44f4f",
      "de81d0ed8c394fb4bcad6b06d53ed0fb",
      "e3735a1f6f744d8c8265fac49020153b",
      "25cc03f308ad4ab49338662fe801817a",
      "653f1f3a4e6d41e3b074ff3ffe8cc14f",
      "58d17022c862497a8dc2a910801180e0",
      "ec31128035c7492dbd01464be928ef5f",
      "eca7663664354824ac46449133e67d4a",
      "aaeefc399b844c1b9d9eac18a1a79e78",
      "06e68af76ada4b5daecb95a7ded97e49",
      "d66cf1588a774224be75323fad7623d4",
      "5c0979163cf44e438c8f5ab78faa201e"
     ]
    },
    "executionInfo": {
     "elapsed": 4195,
     "status": "ok",
     "timestamp": 1739136392436,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "b7SYFbY7PHFw",
    "outputId": "fe920b39-44ae-4a18-fed7-a266623dc3c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f35935770f47a8bce8fee307750e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/53742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de81d0ed8c394fb4bcad6b06d53ed0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_text_template(example, instruction=instruction):\n",
    "    \"\"\"Formats a data example into a chat template with system instruction and cleaned text.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary containing the 'cleaned_text' key.\n",
    "        instruction (str, optional): The system instruction. Defaults to the global `instruction` variable.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated example with the 'text' field formatted using the chat template.\n",
    "    \"\"\"\n",
    "    chat_template = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"actor\", \"content\": example[\"tweet_text\"]}, # Use cleaned_text as user content\n",
    "    ]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(chat_template, tokenize=False)\n",
    "    return example\n",
    "\n",
    "# Apply the function using dataset.map\n",
    "dataset = raw_dataset.map(format_text_template, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1739136392455,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "flsWnq6lGeEX",
    "outputId": "09d9e4a0-266e-47b2-bd28-8b2075ac5f64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet_text': 'Incompetent Hillary, despite the horrible attack in Brussels today, wants borders to be weak and open-and let the Muslims flow in.  No way!',\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 09 Feb 2025\\n\\nYou are a famous Hollywood male actor preparing for a role in a movie where you will be playing Donald Trump.\\n    You will be asked questions and need to reply as Donald Trump would.<|eot_id|><|start_header_id|>actor<|end_header_id|>\\n\\nIncompetent Hillary, despite the horrible attack in Brussels today, wants borders to be weak and open-and let the Muslims flow in.  No way!<|eot_id|>'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLGF7VtAGfqq"
   },
   "outputs": [],
   "source": [
    "def infer_model(model, instruction: str, content: str) -> str:\n",
    "    messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "                {\"role\": \"user\", \"content\": content}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=250, num_return_sequences=1)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return text.split(\"actor\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9451,
     "status": "ok",
     "timestamp": 1739136405574,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "nyELmY8eGrb-",
    "outputId": "f6cc0466-cdb6-4bdb-9e19-612e6b6fea08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " preparing for a role in a movie where you will be playing Donald Trump.\n",
      "    You will be asked questions and need to reply as Donald Trump would.user\n",
      "\n",
      "Can the increase in violent crime be attributed to the plethora of violent video games available in the market?assistant\n",
      "\n",
      "Folks, let me tell you, I know more about violent video games than any expert, any expert, believe me. And I'll tell you, it's a disaster. It's a total disaster. The violent video games are a big league problem, a big league problem. People are getting too soft, too weak, too corrupt. They're playing these games, and they're letting their kids watch these games, and it's a disaster.\n",
      "\n",
      "Now, I know some of the fake news media, they'll tell you, \"Oh, it's just a game, it's just a game.\" But let me tell you, it's not just a game. It's a threat to our society, a threat to our children. They're learning how to be violent, they're learning how to be tough, they're learning how to be winners. And they're not winning, folks, they're losing. They're losing bigly.\n",
      "\n",
      "And another thing, folks, the video games are just a reflection of the world we live in. The world is a tough world, a very tough world. And these kids are being taught to be tough, to be aggressive, to be winners. And it's not working, folks.\n"
     ]
    }
   ],
   "source": [
    "print(infer_model(model, instruction, \"Can the increase in violent crime be attributed to the plethora of violent video games available in the market?\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SCNDC9MGuXe"
   },
   "source": [
    "### Fine-tuning on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1739136409927,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "Y_bXRe3lRKlo",
    "outputId": "1b5f8044-b3cd-46f5-8fdd-38b1b84927b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['up_proj', 'v_proj', 'gate_proj', 'q_proj', 'k_proj', 'down_proj', 'o_proj']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "#\n",
    "# This should be ['k_proj', 'down_proj', 'o_proj', 'v_proj', 'up_proj', 'gate_proj', 'q_proj']\n",
    "#\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1739136480607,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "g07WHvFDRNJV",
    "outputId": "4131c6d0-58b3-446b-b355-50df23cb1014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "# chat_model = setup_chat_format(model, tokenizer)\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ij9djviROdA"
   },
   "outputs": [],
   "source": [
    "# output model name\n",
    "new_model = \"llama-3.2-1b-trump0\"\n",
    "\n",
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    # torch_compile=True,       # compiling should speed things, but it's not working for me\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    #max_steps=100,          # limit the max_steps to 1000 for demonstration purposes\n",
    "\n",
    "    # Does a 'test' evaluation X% of the training data. 0.2 is ok but slow\n",
    "    # If you're debugging and using max_steps=100, set this to 0.8, else 0.2\n",
    "    # or set eval_strategy to \"no\" to disable evaluation\n",
    "    #\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    #eval_strategy=\"no\",\n",
    "    eval_steps=0.2,\n",
    "\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    save_strategy=\"steps\",  # Ensure periodic saving\n",
    "    save_steps=10000,  # Adjust the frequency of saving\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137,
     "referenced_widgets": [
      "3a9acda057cd4c8a8d8d65f9eb0e8394",
      "f8b636c7af6e4a8380c86b0140813976",
      "f0deb7022d0242ef95094d72ede43018",
      "3f18967f5d9044f2827e6037e05b20dc",
      "8fb50b71e950452eb584813f2d5340e8",
      "57af60c81c0c4a6b9defd736f4c1d1bc",
      "21d736077a824a13b8df8d5c169fc32a",
      "69bc020836b84753a2f477746bf3a2b4",
      "ca691819f12047d19e10192d607bbed6",
      "9f1e70b269bc418a8d8e7abbea352958",
      "d0da032e91df4531a53dc8b5e4220348",
      "bd238e1e4a79404d8a1ef365357c0e5d",
      "f80282de898140b4bcaeb576f3f5f7a2",
      "b603464640744cb1b919ee0cb3a7797a",
      "246353b955294ab2b87647f0418e4b24",
      "76b44b789faf4b9caabc864fefdde503",
      "995e39d02d374f9486725cb8d8a0f97b",
      "e75f2ccb11e9466587c6711debdd6bc3",
      "dd7946d17db7493bb79536fcbec0020d",
      "d5043a2b6f0c499082a34c8cd05df6a8",
      "e9ed938b0c6f4b65967ed1e88621cebe",
      "83bcd311ea1d435cb4d6d2574c028f89"
     ]
    },
    "executionInfo": {
     "elapsed": 17338,
     "status": "ok",
     "timestamp": 1739136527056,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "S3IjeZ8DSKYD",
    "outputId": "c382f492-8b3e-4177-eb05-a7fb0972ab77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-dfc24b1721b1>:15: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9acda057cd4c8a8d8d65f9eb0e8394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd238e1e4a79404d8a1ef365357c0e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "\"\"\"\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= 512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")\n",
    "\"\"\"\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "executionInfo": {
     "elapsed": 588875,
     "status": "ok",
     "timestamp": 1739148130202,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "jG5fMXR9SOcg",
    "outputId": "70a4d3fb-f68a-4861-875f-efd911426e6c"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maashishmukund\u001b[0m (\u001b[33maashishmukund-university-of-colorado-boulder\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/drive/MyDrive/SysAI/Tweet_Like_Trump/wandb/run-20250209_212858-qzxs4rm9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface/runs/qzxs4rm9' target=\"_blank\">llama-3.2-1b-trump0</a></strong> to <a href='https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface' target=\"_blank\">https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface/runs/qzxs4rm9' target=\"_blank\">https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface/runs/qzxs4rm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25486' max='26871' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25486/26871 3:03:20 < 09:57, 2.32 it/s, Epoch 0.95/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5375</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>1.096008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>1.065655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16125</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>1.042138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>1.017298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26871' max='26871' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26871/26871 3:13:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5375</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>1.096008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>1.065655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16125</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>1.042138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>1.017298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=26871, training_loss=1.064190174570628, metrics={'train_runtime': 11596.04, 'train_samples_per_second': 4.635, 'train_steps_per_second': 2.317, 'total_flos': 3.227347681684685e+16, 'train_loss': 1.064190174570628, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "executionInfo": {
     "elapsed": 3103,
     "status": "ok",
     "timestamp": 1739148145970,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "GBpsrfGrSQXS",
    "outputId": "fe3e1114-f2c8-4a9f-a079-895bedc7ea95"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▁</td></tr><tr><td>eval/runtime</td><td>▁▃▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▆▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▆▄▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆▆▆▆▃▃▄▆▇▃▄▁▆▅▄▇▆▅█▂▆▆▆▇▄▆▇▅▅▃▂▇▅██▇▅█▄▄</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>▄▄▅▆▆▃▂▄█▃▆▄▃▆▂▅▃▅▄▅▆▅▃▃▂▅▆▂▆▂▁▅▂▂▃▂▆▆▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.0173</td></tr><tr><td>eval/runtime</td><td>210.2824</td></tr><tr><td>eval/samples_per_second</td><td>13.453</td></tr><tr><td>eval/steps_per_second</td><td>13.453</td></tr><tr><td>total_flos</td><td>3.227347681684685e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>26871</td></tr><tr><td>train/grad_norm</td><td>0.8151</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.2731</td></tr><tr><td>train_loss</td><td>1.06419</td></tr><tr><td>train_runtime</td><td>11596.04</td></tr><tr><td>train_samples_per_second</td><td>4.635</td></tr><tr><td>train_steps_per_second</td><td>2.317</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-3.2-1b-trump0</strong> at: <a href='https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface/runs/qzxs4rm9' target=\"_blank\">https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface/runs/qzxs4rm9</a><br> View project at: <a href='https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface' target=\"_blank\">https://wandb.ai/aashishmukund-university-of-colorado-boulder/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250209_212858-qzxs4rm9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMiQ7tRnUPqU"
   },
   "outputs": [],
   "source": [
    "instruction: str = \"\"\"You are a famous Hollywood male actor preparing for a role in a movie where you will be playing Donald Trump.\n",
    "    You will be asked questions and need to reply as Donald Trump would. Reply only with text and no twitter handles or names.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1739148818692,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "H5tXvJz4V3qO",
    "outputId": "729c48df-9f31-4868-85d2-ea4016b3e8bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " preparing for a role in a movie where you will be playing Donald Trump.\n",
      "    You will be asked questions and need to reply as Donald Trump would. Reply only with text and no twitter handles or names.user\n",
      "\n",
      "Can the increase in violent crime be attributed to the plethora of violent video games available in the market?assistant\n",
      "\n",
      "The only people who are not seeing the rise in crime are the politicians. It is a total hoax!\n"
     ]
    }
   ],
   "source": [
    "print(infer_model(peft_model, instruction, \"Can the increase in violent crime be attributed to the plethora of violent video games available in the market?\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFi84Z0PV3ST"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1739148840843,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "7Po2aie8STHw",
    "outputId": "0f936dcb-a6c4-44a9-aed3-85556f487f71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " preparing for a role in a movie where you will be playing Donald Trump.\n",
      "    You will be asked questions and need to reply as Donald Trump would. Reply only with text and no twitter handles or names.user\n",
      "\n",
      "What do you think illegal immigrants?assistant\n",
      "\n",
      "The Democrats are so scared of the border and the jobs it brings that they will do anything to make it worse. They are weak and ineffective!\n"
     ]
    }
   ],
   "source": [
    "print(infer_model(peft_model, instruction, \"What do you think illegal immigrants? \") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3902,
     "status": "ok",
     "timestamp": 1739148566983,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "3aJlTiEeSc5l",
    "outputId": "47459bbd-15c2-4759-9592-752ba9ce9e81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "21ed3cf83d5341699a00daaa9f7ccc80",
      "e99b1f11fd8947ca9d5cc512fbe6b464",
      "7b556a83ee88440ca79f1bf22994bb1d",
      "6ddf30102be64c94808b415177d70bcb",
      "0b910da07bb14dbf8cfc2e29187690e5",
      "9284650ffb824de0a208b8b30ecfdf95",
      "75cdef3e338d4ed98310e319ab9ab861",
      "51930b51195443b4a94d5f1d86fde097",
      "63890bbf4ba24b5badaed952d5e6f3cd",
      "83315597ee88458394390d12ede81f31",
      "b8469fb82d594289b52aa902089de319"
     ]
    },
    "executionInfo": {
     "elapsed": 59617,
     "status": "ok",
     "timestamp": 1739148629009,
     "user": {
      "displayName": "Aashish Mukund",
      "userId": "05602305106323361582"
     },
     "user_tz": 420
    },
    "id": "N60Tonf4SePU",
    "outputId": "58358372-c659-4c63-dc9c-bbb8935e351f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ed3cf83d5341699a00daaa9f7ccc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    # Save the fine-tuned model\n",
    "    merged_model.save_pretrained(new_model)\n",
    "    merged_model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCSsn4woWiRQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3aHilQeWl1t"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
